# Guide de D√©marrage Rapide

Ce guide vous permet de d√©marrer rapidement le syst√®me de recommandation de films.

## üöÄ D√©marrage en 3 √©tapes

### √âtape 1 : Pr√©requis

Assurez-vous d'avoir :
- ‚úÖ Docker Desktop install√© et en cours d'ex√©cution
- ‚úÖ Au moins 8GB de RAM disponible
- ‚úÖ 20GB d'espace disque libre

### √âtape 2 : Initialisation

**Windows (PowerShell) :**
```powershell
.\init.ps1
```

**Linux/Mac :**
```bash
chmod +x init.sh
./init.sh
```

Le script va :
1. D√©marrer PostgreSQL
2. Cr√©er la base de donn√©es MLflow
3. D√©marrer tous les services
4. Initialiser Airflow

### √âtape 3 : V√©rification

Attendez 2-3 minutes que tous les services d√©marrent, puis v√©rifiez :

```bash
docker-compose ps
```

Tous les services doivent √™tre en √©tat "Up" (sauf `airflow-init` et `mlflow-init` qui s'ex√©cutent une seule fois).

## üåê Acc√®s aux Interfaces

Une fois d√©marr√©, acc√©dez aux interfaces :

| Service | URL | Identifiants |
|---------|-----|--------------|
| **Kafka UI** | http://localhost:8080 | - |
| **Spark Master** | http://localhost:8081 | - |
| **Airflow** | http://localhost:8082 | admin / admin |
| **MLflow** | http://localhost:5000 | - |
| **Grafana** | http://localhost:3000 | admin / admin |
| **Prometheus** | http://localhost:9090 | - |
| **FastAPI** | http://localhost:8000 | - |
| **FastAPI Docs** | http://localhost:8000/docs | - |

## ‚úÖ Test de Connectivit√©

Testez que tous les services communiquent correctement :

```bash
docker-compose exec fastapi python test-connections.py
```

Vous devriez voir tous les services marqu√©s comme ‚úÖ.

## üìù Premiers Pas

### 1. Cr√©er un topic Kafka

```bash
docker-compose exec kafka kafka-topics --create \
  --topic films \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1
```

### 2. V√©rifier les DAGs Airflow

1. Allez sur http://localhost:8082
2. Connectez-vous avec admin/admin
3. Vous devriez voir le DAG d'exemple `film_recommendation_pipeline`

### 3. Tester l'API FastAPI

```bash
# Health check
curl http://localhost:8000/health

# Documentation interactive
# Ouvrez http://localhost:8000/docs dans votre navigateur
```

### 4. V√©rifier MLflow

1. Allez sur http://localhost:5000
2. Vous devriez voir l'interface MLflow (vide pour l'instant)

## üõ†Ô∏è Commandes Utiles

### Voir les logs d'un service

```bash
docker-compose logs -f <service-name>
# Exemple: docker-compose logs -f kafka
```

### Red√©marrer un service

```bash
docker-compose restart <service-name>
```

### Arr√™ter tous les services

```bash
docker-compose down
```

### Arr√™ter et supprimer les donn√©es

```bash
docker-compose down -v
```

‚ö†Ô∏è **Attention** : Cela supprime toutes les donn√©es persist√©es !

## üîß Configuration

### Modifier les ressources

√âditez `docker-compose.yml` pour ajuster :
- M√©moire des workers Spark : `SPARK_WORKER_MEMORY`
- Nombre de workers Spark : Ajoutez/supprimez des services `spark-worker-*`
- Ports : Modifiez les mappings de ports

### Ajouter des DAGs Airflow

Placez vos fichiers Python dans `airflow/dags/`. Ils seront automatiquement d√©tect√©s.

### Personnaliser la configuration

Cr√©ez `docker-compose.override.yml` (bas√© sur `docker-compose.override.yml.example`) pour vos personnalisations locales.

## ‚ùì Probl√®mes Courants

### Les services ne d√©marrent pas

1. V√©rifiez que Docker a assez de ressources (Settings > Resources)
2. V√©rifiez les logs : `docker-compose logs <service-name>`
3. V√©rifiez que les ports ne sont pas d√©j√† utilis√©s

### Airflow affiche des erreurs

1. Attendez quelques minutes que l'initialisation se termine
2. V√©rifiez les logs : `docker-compose logs airflow-init`
3. R√©initialisez : `docker-compose up airflow-init`

### Kafka ne fonctionne pas

1. V√©rifiez que Zookeeper est d√©marr√© : `docker-compose ps zookeeper`
2. V√©rifiez les logs : `docker-compose logs kafka zookeeper`

## üìö Prochaines √âtapes

1. **Cr√©er vos DAGs Airflow** : Placez vos workflows dans `airflow/dags/`
2. **D√©velopper l'API** : Modifiez `api/main.py` pour ajouter vos endpoints
3. **Configurer Spark** : Cr√©ez vos jobs Spark dans `spark/jobs/`
4. **Cr√©er des dashboards Grafana** : Importez des dashboards dans `grafana/dashboards/`
5. **Utiliser MLflow** : Int√©grez MLflow dans vos scripts d'entra√Ænement

## üìñ Documentation Compl√®te

Consultez le [README.md](README.md) pour plus de d√©tails et le [CONNECTIONS.md](CONNECTIONS.md) pour les d√©tails de connexion entre services.

