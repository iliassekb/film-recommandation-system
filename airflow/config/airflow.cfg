[core]
dags_folder = /opt/airflow/dags
executor = CeleryExecutor
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow
parallelism = 32
dag_concurrency = 16
dags_are_paused_at_creation = True
load_examples = False
enable_xcom_pickling = True

[webserver]
base_url = http://localhost:8082
web_server_host = 0.0.0.0
web_server_port = 8080
workers = 4
worker_timeout = 120
worker_refresh_batch_size = 1
worker_refresh_interval = 30

[scheduler]
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
run_duration = -1
num_runs = -1
min_file_process_interval = 0
dag_dir_list_interval = 300
print_stats_interval = 30
scheduler_health_check_threshold = 30

[celery]
broker_url = redis://redis:6379/0
result_backend = db+postgresql://airflow:airflow@postgres/airflow
worker_concurrency = 16

[logging]
base_log_folder = /opt/airflow/logs
remote_logging = False
remote_log_conn_id =
remote_base_log_folder =
encrypt_s3_logs = False
logging_level = INFO
fab_logging_level = WARN
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[api]
auth_backends = airflow.api.auth.backend.basic_auth

