"""
Exemple d'utilisation de Spark avec Kafka pour le syst√®me de recommandation
Ce script montre comment lire et traiter des donn√©es depuis Kafka avec Spark
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, window, count
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType

def create_spark_session():
    """Cr√©er une session Spark avec les configurations n√©cessaires"""
    spark = SparkSession.builder \
        .appName("FilmRecommendationKafkaStream") \
        .config("spark.master", "spark://spark-master:7077") \
        .config("spark.sql.warehouse.dir", "/lakehouse/warehouse") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark

def read_from_kafka(spark, kafka_bootstrap_servers, topic):
    """Lire un stream depuis Kafka"""
    df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
        .option("subscribe", topic) \
        .option("startingOffsets", "latest") \
        .load()
    
    return df

def process_film_ratings(df):
    """Traiter les donn√©es de ratings de films"""
    # Sch√©ma pour les ratings
    rating_schema = StructType([
        StructField("user_id", IntegerType(), True),
        StructField("film_id", IntegerType(), True),
        StructField("rating", FloatType(), True),
        StructField("timestamp", TimestampType(), True)
    ])
    
    # Parser les donn√©es JSON
    parsed_df = df.select(
        col("key").cast("string").alias("key"),
        from_json(col("value").cast("string"), rating_schema).alias("data"),
        col("timestamp").alias("kafka_timestamp")
    ).select("key", "data.*", "kafka_timestamp")
    
    # Agr√©gations par fen√™tre temporelle
    windowed_df = parsed_df \
        .withWatermark("timestamp", "10 minutes") \
        .groupBy(
            window(col("timestamp"), "5 minutes"),
            col("film_id")
        ) \
        .agg(
            count("*").alias("rating_count"),
            col("rating").avg().alias("avg_rating")
        )
    
    return windowed_df

def write_to_lakehouse(df, output_path, checkpoint_path):
    """√âcrire les r√©sultats dans le lakehouse (Delta format)"""
    query = df.writeStream \
        .format("delta") \
        .outputMode("append") \
        .option("checkpointLocation", checkpoint_path) \
        .option("path", output_path) \
        .trigger(processingTime="1 minute") \
        .start()
    
    return query

def main():
    """Fonction principale"""
    # Configuration
    kafka_bootstrap_servers = "kafka:29092"
    topic = "film-ratings"
    output_path = "/lakehouse/film_ratings_aggregated"
    checkpoint_path = "/lakehouse/checkpoints/film_ratings"
    
    # Cr√©er la session Spark
    spark = create_spark_session()
    
    print("üì° Lecture depuis Kafka...")
    kafka_df = read_from_kafka(spark, kafka_bootstrap_servers, topic)
    
    print("üîÑ Traitement des donn√©es...")
    processed_df = process_film_ratings(kafka_df)
    
    print("üíæ √âcriture dans le lakehouse...")
    query = write_to_lakehouse(processed_df, output_path, checkpoint_path)
    
    print("‚úÖ Stream d√©marr√©. Appuyez sur Ctrl+C pour arr√™ter.")
    
    try:
        query.awaitTermination()
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Arr√™t du stream...")
        query.stop()

if __name__ == "__main__":
    main()

