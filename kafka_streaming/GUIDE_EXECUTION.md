# Guide d'ex√©cution - Kafka Streaming avec Spark

Ce guide vous explique comment ex√©cuter le projet √©tape par √©tape.

## üìã Pr√©requis

Avant de commencer, assurez-vous d'avoir:

- ‚úÖ **Docker** et **Docker Compose** install√©s
- ‚úÖ **Python 3.7+** install√©
- ‚úÖ **pip** install√©

V√©rifiez vos installations:
```bash
docker --version
docker-compose --version
python --version
pip --version
```

## üöÄ Ex√©cution compl√®te avec Docker

### √âtape 1: Installation des d√©pendances Python

Installez les packages Python n√©cessaires pour le producteur:

```bash
pip install -r requirements.txt
```

### √âtape 2: D√©marrer Kafka et Zookeeper

Dans un premier terminal, d√©marrez les services Kafka:

```bash
docker-compose up -d
```

Ou sur Windows, double-cliquez sur `start_docker.bat`

**V√©rification:**
```bash
docker-compose ps
```

Vous devriez voir:
- `zookeeper` (port 2181)
- `kafka` (port 9092)
- `kafka-ui` (port 8080)

**Attendre quelques secondes** que Kafka soit compl√®tement d√©marr√© (environ 10-15 secondes).

### √âtape 3: Cr√©er les topics Kafka (Optionnel)

Les topics sont cr√©√©s automatiquement, mais vous pouvez les cr√©er manuellement:

```bash
python create_topics.py
```

Ou sur Windows, double-cliquez sur `setup_topics.bat`

Vous devriez voir:
```
‚úÖ Topics cr√©√©s avec succ√®s:
   - clicks
   - views
   - ratings
```

### √âtape 4: D√©marrer le g√©n√©rateur de donn√©es (Producteur)

Dans un **nouveau terminal**, d√©marrez le producteur:

```bash
python producer.py
```

Ou sur Windows, double-cliquez sur `start_producer.bat`

Vous devriez voir:
```
üöÄ D√©marrage du g√©n√©rateur de streaming...
üìä Envoi de donn√©es aux topics: clicks, views, ratings
‚èπÔ∏è  Appuyez sur Ctrl+C pour arr√™ter

‚úÖ Donn√©es envoy√©es au topic clicks: click
‚úÖ Donn√©es envoy√©es au topic views: view
‚úÖ Donn√©es envoy√©es au topic ratings: rating
...
```

**Laissez ce terminal ouvert** - il envoie continuellement des donn√©es √† Kafka.

### √âtape 5: D√©marrer le consumer Spark Streaming

Dans un **troisi√®me terminal**, d√©marrez le consumer Spark:

**Option A: Avec Docker (Recommand√©)**
```bash
docker-compose -f docker-compose.yml -f docker-compose.spark.yml up spark-consumer
```

Ou sur Windows, double-cliquez sur `start_spark_docker.bat`

**Option B: En local (si vous avez Spark install√©)**
```bash
python consumer_spark.py
```

Ou sur Windows, double-cliquez sur `start_consumer.bat`

Vous devriez voir:
```
üöÄ D√©marrage du consumer Spark Streaming...
üìä Consommation des topics: clicks, views, ratings
üíæ Sauvegarde dans: data/parquet/

üì• Traitement du stream 'clicks'...
üì• Traitement du stream 'views'...
üì• Traitement du stream 'ratings'...

‚úÖ Tous les streams sont actifs!
‚èπÔ∏è  Appuyez sur Ctrl+C pour arr√™ter
```

### √âtape 6: V√©rifier que tout fonctionne

#### A. V√©rifier dans Kafka UI

Ouvrez votre navigateur sur: **http://localhost:8080**

Vous pouvez:
- Voir les 3 topics: `clicks`, `views`, `ratings`
- Voir les messages qui arrivent en temps r√©el
- Voir les consommateurs actifs

#### B. V√©rifier les fichiers Parquet

Les fichiers Parquet sont cr√©√©s toutes les 10 secondes dans:
```
data/parquet/clicks/
data/parquet/views/
data/parquet/ratings/
```

Apr√®s quelques minutes, vous devriez voir des fichiers `.parquet` appara√Ætre.

Pour v√©rifier (sur Windows PowerShell):
```powershell
Get-ChildItem -Recurse data/parquet/ | Select-Object Name, Length, LastWriteTime
```

#### C. V√©rifier les logs

**Logs Kafka:**
```bash
docker-compose logs -f kafka
```

**Logs Spark Consumer:**
```bash
docker-compose logs -f spark-consumer
```

## üìä Ordre d'ex√©cution recommand√©

```
1. Terminal 1: docker-compose up -d           (Kafka/Zookeeper)
   ‚Üì Attendre 10-15 secondes
   
2. Terminal 2: python producer.py            (G√©n√©rateur de donn√©es)
   ‚Üì Laissez tourner
   
3. Terminal 3: docker-compose -f docker-compose.yml -f docker-compose.spark.yml up spark-consumer
   ‚Üì (Spark Consumer - lit depuis Kafka et sauvegarde en Parquet)
```

## ‚èπÔ∏è Arr√™ter l'application

### Arr√™ter le producteur
Dans le terminal du producteur, appuyez sur: `Ctrl+C`

### Arr√™ter le consumer Spark
Dans le terminal du consumer, appuyez sur: `Ctrl+C`

### Arr√™ter Kafka
```bash
docker-compose down
```

Ou pour arr√™ter tout:
```bash
docker-compose -f docker-compose.yml -f docker-compose.spark.yml down
```

## üîÑ Red√©marrer apr√®s arr√™t

1. Red√©marrer Kafka:
   ```bash
   docker-compose up -d
   ```

2. Red√©marrer le producteur:
   ```bash
   python producer.py
   ```

3. Red√©marrer le consumer Spark:
   ```bash
   docker-compose -f docker-compose.yml -f docker-compose.spark.yml up spark-consumer
   ```

**Note:** Les fichiers Parquet existants ne seront pas √©cras√©s, les nouvelles donn√©es seront ajout√©es.

## üêõ D√©pannage

### Probl√®me: Kafka ne d√©marre pas

**Solution:**
1. V√©rifiez que les ports 9092, 2181, 8080 ne sont pas utilis√©s
2. Arr√™tez tous les containers: `docker-compose down`
3. Red√©marrez: `docker-compose up -d`
4. Attendez 15-20 secondes que Kafka soit compl√®tement d√©marr√©

### Probl√®me: Le producteur ne peut pas se connecter √† Kafka

**Erreur typique:** `NoBrokersAvailable`

**Solution:**
1. V√©rifiez que Kafka est d√©marr√©: `docker-compose ps`
2. Attendez quelques secondes de plus
3. V√©rifiez les logs: `docker-compose logs kafka`

### Probl√®me: "Failed to find data source: kafka"

**Solution:**
1. Le package Kafka sera t√©l√©charg√© automatiquement lors de la premi√®re ex√©cution
2. Assurez-vous d'avoir une connexion Internet
3. Le t√©l√©chargement peut prendre quelques minutes la premi√®re fois
4. Pour forcer une version sp√©cifique de Spark, d√©finissez la variable d'environnement:
   ```bash
   set SPARK_VERSION=3.5.0  # Windows
   export SPARK_VERSION=3.5.0  # Linux/Mac
   ```
5. Voir `TROUBLESHOOTING.md` pour plus de d√©tails

### Probl√®me: Spark ne peut pas se connecter √† Kafka

**Solution:**
1. V√©rifiez que Kafka est d√©marr√©: `docker-compose ps`
2. V√©rifiez que les deux containers sont sur le m√™me r√©seau
3. V√©rifiez les logs Spark: `docker-compose logs spark-consumer`
4. Assurez-vous que la variable d'environnement `KAFKA_BOOTSTRAP_SERVERS=kafka:29092` est bien d√©finie

### Probl√®me: Aucun fichier Parquet n'est cr√©√©

**Solution:**
1. V√©rifiez que le producteur envoie bien des donn√©es (regardez les logs)
2. V√©rifiez dans Kafka UI que les messages arrivent bien dans les topics
3. Attendez au moins 10 secondes (les fichiers sont cr√©√©s toutes les 10 secondes)
4. V√©rifiez les permissions des dossiers `data/` et `checkpoints/`
5. V√©rifiez les logs Spark pour voir s'il y a des erreurs

### Probl√®me: "Topic does not exist"

**Solution:**
1. Cr√©ez les topics manuellement: `python create_topics.py`
2. Ou attendez que le producteur les cr√©e automatiquement (auto-cr√©ation activ√©e)

## üìà Monitoring et visualisation

### Kafka UI
Acc√©dez √† http://localhost:8080 pour:
- Voir tous les topics
- Voir les messages en temps r√©el
- Voir les m√©triques des topics
- Voir les consommateurs actifs

### V√©rifier les messages Kafka via ligne de commande

**Voir les messages du topic clicks:**
```bash
docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic clicks --from-beginning
```

**Voir les messages du topic views:**
```bash
docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic views --from-beginning
```

**Voir les messages du topic ratings:**
```bash
docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic ratings --from-beginning
```

### Lister les topics
```bash
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092
```

## üìù Exemple de workflow complet

```bash
# 1. Installer les d√©pendances
pip install -r requirements.txt

# 2. D√©marrer Kafka
docker-compose up -d

# 3. Attendre 15 secondes que Kafka d√©marre
sleep 15  # Sur Windows PowerShell: Start-Sleep -Seconds 15

# 4. Cr√©er les topics (optionnel)
python create_topics.py

# 5. Dans un nouveau terminal: D√©marrer le producteur
python producer.py

# 6. Dans un autre terminal: D√©marrer le consumer Spark
docker-compose -f docker-compose.yml -f docker-compose.spark.yml up spark-consumer

# 7. Laisser tourner pendant quelques minutes...

# 8. Ouvrir http://localhost:8080 pour voir les donn√©es dans Kafka UI

# 9. V√©rifier les fichiers Parquet cr√©√©s
dir data\parquet\clicks
dir data\parquet\views
dir data\parquet\ratings
```

## ‚úÖ Checklist de v√©rification

Avant de consid√©rer que tout fonctionne, v√©rifiez:

- [ ] Kafka est d√©marr√© (`docker-compose ps`)
- [ ] Kafka UI est accessible (http://localhost:8080)
- [ ] Le producteur envoie des donn√©es (logs visibles)
- [ ] Les topics existent dans Kafka UI (`clicks`, `views`, `ratings`)
- [ ] Des messages arrivent dans les topics (visible dans Kafka UI)
- [ ] Le consumer Spark est d√©marr√© et affiche "‚úÖ Tous les streams sont actifs!"
- [ ] Des fichiers Parquet sont cr√©√©s dans `data/parquet/`
- [ ] Les fichiers Parquet sont mis √† jour r√©guli√®rement (toutes les 10 secondes)

## üéØ R√©sultat attendu

Apr√®s avoir suivi ces √©tapes, vous devriez avoir:

1. ‚úÖ Des donn√©es g√©n√©r√©es et envoy√©es √† Kafka en temps r√©el
2. ‚úÖ 3 topics Kafka actifs avec des donn√©es
3. ‚úÖ Spark Streaming qui consomme les donn√©es depuis Kafka
4. ‚úÖ Des fichiers Parquet cr√©√©s et mis √† jour toutes les 10 secondes
5. ‚úÖ Une interface web (Kafka UI) pour visualiser les donn√©es

---

**Besoin d'aide?** Consultez le fichier `DOCKER_GUIDE.md` pour plus de d√©tails sur Docker, ou `README.md` pour la documentation compl√®te du projet.


