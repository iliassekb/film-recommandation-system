# Projet Kafka Streaming avec Spark

Ce projet permet de gÃ©nÃ©rer des donnÃ©es de streaming (clicks, views, ratings), de les envoyer Ã  Kafka via 3 topics, et de les consommer avec Spark Streaming pour les sauvegarder dans des fichiers Parquet.

## Architecture

```
GÃ©nÃ©rateur de donnÃ©es â†’ Kafka (3 topics) â†’ Spark Streaming â†’ Fichiers Parquet
```

- **Topics Kafka**: `clicks`, `views`, `ratings`
- **Fichiers Parquet**: `data/parquet/clicks/`, `data/parquet/views/`, `data/parquet/ratings/`

> ğŸš€ **DÃ©marrage rapide:** Consultez [QUICK_START.md](QUICK_START.md)  
> ğŸ“– **Guide d'exÃ©cution dÃ©taillÃ©:** Consultez [GUIDE_EXECUTION.md](GUIDE_EXECUTION.md)  
> ğŸ³ **Guide Docker:** Consultez [DOCKER_GUIDE.md](DOCKER_GUIDE.md)

## PrÃ©requis

### Option 1: Avec Docker (RecommandÃ©)

1. **Docker** et **Docker Compose**
   ```bash
   docker --version
   docker-compose --version
   ```

2. **Python 3.7+** (pour le producteur qui tourne localement)

### Option 2: Installation locale

1. **Java JDK 8 ou supÃ©rieur**
   ```bash
   java -version
   ```

2. **Kafka** (version 2.x ou 3.x)
   - TÃ©lÃ©charger depuis https://kafka.apache.org/downloads
   - Extraire et dÃ©marrer Zookeeper et Kafka

3. **Apache Spark 3.x**
   - TÃ©lÃ©charger depuis https://spark.apache.org/downloads.html

4. **Python 3.7+**

## Installation

### Option 1: Avec Docker

1. Installer les dÃ©pendances Python pour le producteur:
   ```bash
   pip install -r requirements.txt
   ```

2. DÃ©marrer Kafka et Zookeeper avec Docker:
   ```bash
   docker-compose up -d
   ```

   Cela dÃ©marre:
   - Zookeeper (port 2181)
   - Kafka (port 9092)
   - Kafka UI (port 8080) - Interface web pour visualiser les topics

3. VÃ©rifier que les conteneurs sont dÃ©marrÃ©s:
   ```bash
   docker-compose ps
   ```

4. CrÃ©er les topics Kafka (optionnel, auto-crÃ©ation activÃ©e):
   ```bash
   python create_topics.py
   ```

### Option 2: Installation locale

1. Installer les dÃ©pendances Python:
   ```bash
   pip install -r requirements.txt
   ```

2. Configurer les variables d'environnement pour Spark:
   ```bash
   # Windows
   set SPARK_HOME=C:\path\to\spark
   
   # Linux/Mac
   export SPARK_HOME=/path/to/spark
   ```

3. DÃ©marrer Zookeeper:
   ```bash
   bin\zookeeper-server-start.bat config\zookeeper.properties
   ```

4. DÃ©marrer Kafka:
   ```bash
   bin\kafka-server-start.bat config\server.properties
   ```

5. CrÃ©er les 3 topics:
   ```bash
   bin\kafka-topics.bat --create --topic clicks --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
   bin\kafka-topics.bat --create --topic views --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
   bin\kafka-topics.bat --create --topic ratings --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
   ```

## Utilisation

### Option 1: Avec Docker (Kafka + Spark)

1. **DÃ©marrer Kafka et Zookeeper**:
   ```bash
   docker-compose up -d
   ```

2. **DÃ©marrer le consumer Spark Streaming dans Docker**:
   ```bash
   docker-compose -f docker-compose.yml -f docker-compose.spark.yml up spark-consumer
   ```

   Ou construire et lancer sÃ©parÃ©ment:
   ```bash
   docker build -f Dockerfile.spark -t spark-consumer .
   docker run --network kafka-streaming_kafka-network -e KAFKA_BOOTSTRAP_SERVERS=kafka:29092 -v ${PWD}/data:/opt/bitnami/spark/work/data -v ${PWD}/checkpoints:/opt/bitnami/spark/work/checkpoints spark-consumer
   ```

3. **DÃ©marrer le producteur localement** (dans un autre terminal):
   ```bash
   python producer.py
   ```

### Option 2: Tout en local

1. **DÃ©marrer le producteur** (gÃ©nÃ©rateur de donnÃ©es):
   ```bash
   python producer.py
   ```

2. **DÃ©marrer le consumer Spark Streaming**:
   ```bash
   python consumer_spark.py
   ```

### Interface Kafka UI (avec Docker)

Si vous utilisez Docker, vous pouvez visualiser les topics et messages via l'interface web:
- URL: http://localhost:8080
- Permet de voir les topics, les messages, et les consommateurs

## Structure des donnÃ©es

### Clicks
- `event_type`: "click"
- `user_id`: ID de l'utilisateur (1-1000)
- `item_id`: ID de l'item (1-500)
- `timestamp`: Horodatage ISO
- `page_url`: URL de la page
- `click_duration`: DurÃ©e du clic en secondes

### Views
- `event_type`: "view"
- `user_id`: ID de l'utilisateur (1-1000)
- `item_id`: ID de l'item (1-500)
- `timestamp`: Horodatage ISO
- `view_duration`: DurÃ©e de la vue en secondes
- `device_type`: Type d'appareil (mobile/desktop/tablet)

### Ratings
- `event_type`: "rating"
- `user_id`: ID de l'utilisateur (1-1000)
- `item_id`: ID de l'item (1-500)
- `timestamp`: Horodatage ISO
- `rating`: Note (1-5)
- `review_text`: Texte de la revue

## Structure des fichiers

```
kafka_streaming/
â”œâ”€â”€ producer.py                 # GÃ©nÃ©rateur de donnÃ©es et producteur Kafka
â”œâ”€â”€ consumer_spark.py           # Consumer Spark Streaming
â”œâ”€â”€ create_topics.py            # Script pour crÃ©er les topics Kafka
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ README.md                   # Ce fichier
â”œâ”€â”€ docker-compose.yml          # Configuration Docker pour Kafka/Zookeeper
â”œâ”€â”€ docker-compose.spark.yml    # Extension pour Spark consumer
â”œâ”€â”€ Dockerfile.spark            # Dockerfile pour Spark Streaming
â”œâ”€â”€ .gitignore                  # Fichiers ignorÃ©s par Git
â”œâ”€â”€ data/
â”‚   â””â”€â”€ parquet/
â”‚       â”œâ”€â”€ clicks/            # Fichiers Parquet pour les clicks
â”‚       â”œâ”€â”€ views/             # Fichiers Parquet pour les views
â”‚       â””â”€â”€ ratings/           # Fichiers Parquet pour les ratings
â””â”€â”€ checkpoints/               # Checkpoints Spark Streaming
    â”œâ”€â”€ clicks/
    â”œâ”€â”€ views/
    â””â”€â”€ ratings/
```

## Commandes Docker utiles

### DÃ©marrer les services
```bash
docker-compose up -d
```

### ArrÃªter les services
```bash
docker-compose down
```

### Voir les logs
```bash
docker-compose logs -f kafka
docker-compose logs -f spark-consumer
```

### ArrÃªter uniquement Spark
```bash
docker-compose -f docker-compose.yml -f docker-compose.spark.yml stop spark-consumer
```

### Supprimer les donnÃ©es (volumes)
```bash
docker-compose down -v
```

## Notes

- Les fichiers Parquet sont sauvegardÃ©s toutes les 10 secondes (configurable dans `consumer_spark.py`)
- Les checkpoints Spark sont sauvegardÃ©s dans le dossier `checkpoints/` pour permettre la reprise aprÃ¨s interruption
- Pour modifier la frÃ©quence de gÃ©nÃ©ration des donnÃ©es, changez le paramÃ¨tre `interval` dans `producer.py`

