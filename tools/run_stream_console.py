#!/usr/bin/env python3
"""
Script Python autonome pour lancer stream_kafka_console.py avec les bonnes configurations.
Peut √™tre ex√©cut√© directement ou depuis un conteneur Docker.

Usage:
    # Mode local (recommand√© pour ressources limit√©es)
    docker-compose exec spark-master python3 /tmp/run_stream_console.py --mode local
    
    # Mode cluster (n√©cessite JARs install√©s)
    docker-compose exec spark-master python3 /tmp/run_stream_console.py --mode cluster
    
    # Depuis la machine h√¥te (Windows/Linux/Mac)
    docker cp tools/run_stream_console.py spark-master:/tmp/
    docker-compose exec spark-master python3 /tmp/run_stream_console.py --mode local
"""

import os
import sys
import subprocess
import argparse


def main():
    parser = argparse.ArgumentParser(
        description="Lancer stream_kafka_console.py avec les bonnes configurations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Mode local (recommand√©)
  python3 run_stream_console.py --mode local
  
  # Mode cluster (n√©cessite JARs install√©s)
  python3 run_stream_console.py --mode cluster
  
  # Depuis machine h√¥te avec Docker
  docker cp tools/run_stream_console.py spark-master:/tmp/
  docker-compose exec spark-master python3 /tmp/run_stream_console.py --mode local
        """
    )
    parser.add_argument(
        "--mode",
        choices=["local", "cluster"],
        default="local",
        help="Mode d'ex√©cution: local (driver uniquement) ou cluster (workers) (default: local)"
    )
    parser.add_argument(
        "--master",
        default=None,
        help="Spark master URL (override mode default)"
    )
    
    args = parser.parse_args()
    
    # Configuration par d√©faut
    kafka_bootstrap = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")
    storage_format = os.getenv("STORAGE_FORMAT", "parquet")
    lakehouse_path = os.getenv("LAKEHOUSE_PATH", "/data")
    
    # D√©terminer le master
    if args.master:
        spark_master = args.master
    elif args.mode == "local":
        spark_master = "local[2]"
    else:
        spark_master = os.getenv("SPARK_MASTER", "spark://spark-master:7077")
    
    # Chemin du script √† ex√©cuter
    # Dans le conteneur, le script est mont√© √† /opt/spark/jobs/stream_kafka_console.py
    possible_paths = [
        "/opt/spark/jobs/stream_kafka_console.py",  # Chemin dans le conteneur (mont√©)
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "spark", "jobs", "stream_kafka_console.py"),
        "spark/jobs/stream_kafka_console.py",  # Chemin relatif depuis tools/
    ]
    
    job_script = None
    for path in possible_paths:
        abs_path = os.path.abspath(path)
        if os.path.exists(abs_path):
            job_script = abs_path
            break
    
    if not job_script or not os.path.exists(job_script):
        print("‚ùå Erreur: Script stream_kafka_console.py introuvable.")
        print("   Chemins essay√©s:")
        for path in possible_paths:
            print(f"   - {os.path.abspath(path)}")
        sys.exit(1)
    
    # Construire la commande spark-submit
    spark_submit_cmd = [
        "/opt/spark/bin/spark-submit",
        "--master", spark_master,
        "--deploy-mode", "client",
        "--conf", "spark.sql.adaptive.enabled=true",
        "--conf", "spark.sql.adaptive.coalescePartitions.enabled=true",
    ]
    
    # Ajouter --packages pour mode local
    if args.mode == "local":
        spark_submit_cmd.extend([
            "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0"
        ])
    
    # Ajouter le script
    spark_submit_cmd.append(job_script)
    
    # Afficher les informations
    print("=" * 80)
    print("üöÄ D√©marrage du streaming Kafka ‚Üí Console")
    print(f"   Mode: {args.mode.upper()}")
    print(f"   Spark Master: {spark_master}")
    print(f"   Kafka: {kafka_bootstrap}")
    print(f"   Topics: events_views, events_clicks, events_ratings")
    print("=" * 80)
    print()
    
    if args.mode == "local":
        print("‚ÑπÔ∏è  Mode LOCAL: ex√©cution sur le driver uniquement")
        print("   Les JARs Kafka seront t√©l√©charg√©s automatiquement via --packages")
        print("   Parfait pour ressources limit√©es (4 cores, 2GB RAM)")
        print()
    else:
        print("‚ÑπÔ∏è  Mode CLUSTER: assurez-vous que les JARs Kafka sont install√©s sur tous les workers")
        print("   Ex√©cutez d'abord: scripts/install_kafka_jars.ps1 ou scripts/install_kafka_jars.sh")
        print()
    
    print("üì° Les √©v√©nements seront affich√©s dans la console toutes les 2 secondes")
    print("   Appuyez sur Ctrl+C pour arr√™ter")
    print()
    print("-" * 80)
    print()
    
    # D√©finir les variables d'environnement
    env = os.environ.copy()
    env["KAFKA_BOOTSTRAP_SERVERS"] = kafka_bootstrap
    env["STORAGE_FORMAT"] = storage_format
    env["LAKEHOUSE_PATH"] = lakehouse_path
    
    # Ex√©cuter spark-submit
    try:
        subprocess.run(spark_submit_cmd, env=env, check=True)
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Arr√™t du streaming...")
        sys.exit(0)
    except FileNotFoundError:
        print(f"\n‚ùå Erreur: spark-submit introuvable √† {spark_submit_cmd[0]}")
        print("   Assurez-vous d'ex√©cuter ce script dans le conteneur Spark")
        print("   Ou utilisez: docker-compose exec spark-master python3 /tmp/run_stream_console.py")
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå Erreur lors de l'ex√©cution: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

